{"cells":[{"metadata":{},"cell_type":"markdown","source":"<h1>**PZ.U17 - Marketing bankowy.**</h1>\n<ul>\n    <li>Sebastian Smoliński - nr indeksu </li>\n    <li>Oleksandr Drobinin - nr indeksu</li>\n    <li>Poniższe analizy opierają sie na pliku bank.csv dostępnym [tutaj](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nprint(os.listdir(\"../input/\"))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"04ecd3cd795b6e4cf4ff048c1f00601ef69f8e95","_cell_guid":"55cf54c6-60c1-4def-a8a2-996d89cf8d96","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom plotly import tools\nimport plotly.plotly as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n#Poniższa linia dla Jupyter Notebooka\ninit_notebook_mode(connected=True)\n#Alternatywne wczytanie pliku danych z tego samego folderu lub z innego miejsca przy podaniu całej ścieżki\n#df = pd.read_csv(\"bank.csv\")\n\nMAIN_PATH = '../input/bank-marketing-dataset/'#bank-marketing/ #bank-marketing-dataset/\ndf = pd.read_csv(MAIN_PATH +'bank.csv') #bank-additional-full.csv #bank.csv\nterm_deposits = df.copy()\n#Wyświetlenie części \ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"ae1d01a87f5423c152ee14df074f4a6da0f508fc","_cell_guid":"82a65d1c-2d52-46d2-a2f9-cc201291af9f"},"cell_type":"markdown","source":"\n<h3> Podstawowe założenia </h3>\n<a id=\"overall_analysis\"></a>\n***\n<ul>\n<li type=\"square\">Na potrzeby dalszych operacji przy budowaniu modelów do klasyfikacji usunięto kolumnę \"duration\", gdyż zbyt mocno oddziaływała wynik działania algorytmów. </li><br>\n</ul>\n\n\n"},{"metadata":{"_uuid":"001b540f01383cdf4817cf2a3ae1d0467a35bf0d","_cell_guid":"49bb0e1d-ea5d-43fb-a62e-89ec2cf0248e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.describe()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b881241ef51fdc4fd3f59b245b78afe2c60bf796","_cell_guid":"22f978f6-39e7-45ea-8061-04b23bce004c"},"cell_type":"markdown","source":"Nie ma żadnych pustych wartości"},{"metadata":{"_uuid":"1bce354de17ec340c6e9fe7f9aff1b7ffaeef823","_cell_guid":"0bf2d612-22d9-4263-997f-7842032223d4","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"92706f68c12026775143ec8b6f200cc94f739edd","_cell_guid":"10648601-36e1-464f-83e0-040b59326d1e","trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Rozkład danych\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\n\ndf.hist(bins=20, figsize=(14,10), color='#E14906')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"869d4858016402b8a8cd0c6eff5317a59ee3415e"},"cell_type":"code","source":"df['deposit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e95bc86ba09f1bd6b04dabbb7fcf1279293c9165"},"cell_type":"code","source":"# plt.style.use('dark_background')\nfig = plt.figure(figsize=(20,20))\nax1 = fig.add_subplot(221)\nax2 = fig.add_subplot(222)\nax3 = fig.add_subplot(212)\n\ng = sns.boxplot(x=\"default\", y=\"balance\", hue=\"deposit\",\n                    data=df, palette=\"muted\", ax=ax1)\n\ng.set_title(\"Stan konta w zalężności od długości lokaty\")\n\n# ax.set_xticklabels(df[\"default\"].unique(), rotation=45, rotation_mode=\"anchor\")\n\ng1 = sns.boxplot(x=\"job\", y=\"balance\", hue=\"deposit\",\n                 data=df, palette=\"RdBu\", ax=ax2)\n\ng1.set_xticklabels(df[\"job\"].unique(), rotation=90, rotation_mode=\"anchor\")\ng1.set_title(\"Rodzaj pracy w zależności od długości trwania lokaty\")\n\ng2 = sns.violinplot(data=df, x=\"education\", y=\"balance\", hue=\"deposit\", palette=\"RdBu_r\")\n\ng2.set_title(\"Stan konta w zależności od wykształcenia\")\n\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1281bd3cca6c5050493a79c9b6c763abdb2de373"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2ba88aeb22bcb93ee3810c86c494de31cda39844"},"cell_type":"markdown","source":"<h3> Analiza struktury zatrudnienia </h3>\n<ul> \n    <li><b>Wiek </b>  Tak jak można przewidywać, najniższy średni wiek jest wśród tudentów oraz najwyższy wśród emerytów.</li>\n    <li><b> Stan konta: </b> Kadra zarządzająca i emeryci to grupy najbardziej majętne. </li>\n    </ul>"},{"metadata":{"trusted":true,"_uuid":"e6f0164ced43c674d386217be2b8bd3c082269dc","_kg_hide-input":true},"cell_type":"code","source":"# Usunięcie nieznanych form zatrudnienia\ndf = df.drop(df.loc[df[\"job\"] == \"unknown\"].index)\n\n#Złączenie administracji i kadry zarządzającej\nlst = [df]\n\nfor col in lst:\n    col.loc[col[\"job\"] == \"admin.\", \"job\"] = \"management\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e6aeb678daa10227dfd875d69721184b6edcd4d4","_kg_hide-input":true},"cell_type":"code","source":"# Sprawdzamy, które zawody mają lepszą sytuacje finansową\n\nsuscribed_df = df.loc[df[\"deposit\"] == \"yes\"]\n\noccupations = df[\"job\"].unique().tolist()\n\n# Stan konta po zawodach\nmanagement = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"management\"].values\ntechnician = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"technician\"].values\nservices = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"services\"].values\nretired = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"retired\"].values\nblue_collar = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"blue-collar\"].values\nunemployed = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"unemployed\"].values\nentrepreneur = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"entrepreneur\"].values\nhousemaid = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"housemaid\"].values\nself_employed = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"self-employed\"].values\nstudent = suscribed_df[\"age\"].loc[suscribed_df[\"job\"] == \"student\"].values\n\n\nages = [management, technician, services, retired, blue_collar, unemployed, \n         entrepreneur, housemaid, self_employed, student]\n\ncolors = ['rgba(93, 164, 214, 0.5)', 'rgba(255, 144, 14, 0.5)',\n          'rgba(44, 160, 101, 0.5)', 'rgba(255, 65, 54, 0.5)', \n          'rgba(207, 114, 255, 0.5)', 'rgba(127, 96, 0, 0.5)',\n         'rgba(229, 126, 56, 0.5)', 'rgba(229, 56, 56, 0.5)',\n         'rgba(174, 229, 56, 0.5)', 'rgba(229, 56, 56, 0.5)']\n\ntraces = []\n\nfor xd, yd, cls in zip(occupations, ages, colors):\n        traces.append(go.Box(\n            y=yd,\n            name=xd,\n            boxpoints='all',\n            jitter=0.5,\n            whiskerwidth=0.2,\n            fillcolor=cls,\n            marker=dict(\n                size=2,\n            ),\n            line=dict(width=1),\n        ))\n\nlayout = go.Layout(\n    title='Rozkład wiekowy w poszczególnych zawodach',\n    yaxis=dict(\n        autorange=True,\n        showgrid=True,\n        zeroline=True,\n        dtick=5,\n        gridcolor='rgb(255, 255, 255)',\n        gridwidth=1,\n        zerolinecolor='rgb(255, 255, 255)',\n        zerolinewidth=2,\n    ),\n    margin=dict(\n        l=40,\n        r=30,\n        b=80,\n        t=100,\n    ),\n    paper_bgcolor='rgb(224,255,246)',\n    plot_bgcolor='rgb(251,251,251)',\n    showlegend=False\n)\n\nfig = go.Figure(data=traces, layout=layout)\niplot(fig)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"07a2ba3b867c3c69782cf0a9444c134bd835a416"},"cell_type":"markdown","source":"<h3> Stan cywilny </h3>"},{"metadata":{"trusted":true,"_uuid":"233f669cdb562c52a0edab979c2933c89451d0c1","_kg_hide-input":true},"cell_type":"code","source":"df['marital'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"632f06ccddbe410aaccf6b7a432ffe56108aa47f","_kg_hide-input":true},"cell_type":"code","source":"df['marital'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c33cfa8f06d7a68f61e2e9eb2658bb021250680","_kg_hide-input":true},"cell_type":"code","source":"df['marital'].value_counts().tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7b9af53b779d764cd7d020599b4f93fd77ca6da7","_kg_hide-input":true},"cell_type":"code","source":"# Stan konta w zależności od stanu cywilnego\nsingle = df['balance'].loc[df['marital'] == 'single'].values\nmarried = df['balance'].loc[df['marital'] == 'married'].values\ndivorced = df['balance'].loc[df['marital'] == 'divorced'].values\n\n\nsingle_dist = go.Histogram(\n    x=single,\n    histnorm='density', \n    name='single',\n    marker=dict(\n        color='#6E6E6E'\n    )\n)\n\n\nmarried_dist = go.Histogram(\n    x=married,\n    histnorm='density', \n    name='married',\n    marker=dict(\n        color='#2E9AFE'\n    )\n)\n\ndivorced_dist = go.Histogram(\n    x=divorced,\n    histnorm='density', \n    name='divorced',\n    marker=dict(\n        color='#FA5858'\n    )\n)\n\n\nfig = tools.make_subplots(rows=3, print_grid=False)\n\nfig.append_trace(single_dist, 1, 1)\nfig.append_trace(married_dist, 2, 1)\nfig.append_trace(divorced_dist, 3, 1)\n\n\nfig['layout'].update(showlegend=False, title=\"Zarobki w zależności od stanu cywilnego\",\n                    height=1000, width=800)\n\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9893d3fe4e2b230ea41172200470b03b77123a18","_kg_hide-input":true},"cell_type":"code","source":"\ndf = df.drop(df.loc[df[\"education\"] == \"unknown\"].index)\ndf['education'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c36347e88b4ab0866d6a3bd547fc0db87324cda8"},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"46bdebb06eb0c68128098fb9804b301a4eb065f6"},"cell_type":"code","source":"#Macierz korelacji\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfig = plt.figure(figsize=(20,20))\ndf['deposit'] = LabelEncoder().fit_transform(df['deposit'])\n\n# Separate both dataframes into \nnumeric_df = df.select_dtypes(exclude=\"object\")\ncategorical_df = df.select_dtypes(include=\"object\")\n\ncorr_numeric = numeric_df.corr()\ncorr_categorical = categorical_df.corr()\n\nsns.heatmap(corr_numeric, cbar=True, cmap=\"RdBu_r\")\nplt.title(\"Macierz korelacji\", fontsize=16)\nplt.show()\n\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6715e27546ccd5cf59c97c677819c170275ec89a"},"cell_type":"markdown","source":"<h2> <b>Model do klasyfikacji:</b> </h2>"},{"metadata":{"trusted":true,"_uuid":"4a1227158f08fd3d7d23b14a2b5cca68fc3e8c32"},"cell_type":"code","source":"dep = term_deposits['deposit']\nterm_deposits.drop(labels=['deposit'], axis=1,inplace=True)\nterm_deposits.insert(0, 'deposit', dep)\nterm_deposits.head()\n# Sprawdzamy i usuwamy kolumny, które zaburzają wynik działania klasyfikatorów przez zbyt duże oddziaływanie na zbiór\nterm_deposits[\"housing\"].value_counts()/len(term_deposits)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1e8365a2431959fa9ce1c52568e7d66aee7a756c"},"cell_type":"code","source":"term_deposits[\"loan\"].value_counts()/len(term_deposits)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"274b9ec411b129f47c0197b71d39753a92087514"},"cell_type":"markdown","source":"## Próbkowanie powłokowe: \n\nW tym miejscu użyliśmy funkcji z pozycji podanej w literaturze, aby podzielić dane do zbiorów w odpowiednich proporcjach tzn. jeśli w pierwotnym modelu \"loan\" miało 87% \"no\" i 13% \"yes\", to i w naszych modelach treningowym i testowym chcemy takie miec, aby wyniki odzwierciedlały jak najlepiej zbiór pierwotny. Dodatkowo należy zaimplementować walidacje krzyżową."},{"metadata":{"trusted":true,"_uuid":"e48052d017060f95d5adf6c9423a17d12256c269"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\n# Dzielimy zbiór na treningowy i testowy; wartość random_state wynosi 42, bo taką znaleźliśmy jaką powszechnie używaną do inicjalizowania wewnetrznego RNG  innych opracowaniach.\nstratified = StratifiedShuffleSplit(n_splits=20, test_size=0.4, random_state=42)\n\nfor train_set, test_set in stratified.split(term_deposits, term_deposits[\"loan\"]):\n    stratified_train = term_deposits.loc[train_set]\n    stratified_test = term_deposits.loc[test_set]\n    \nstratified_train[\"loan\"].value_counts()/len(df)\nstratified_test[\"loan\"].value_counts()/len(df)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"692970c0c6d3a27d0c266a6cbe0b25bee5536884"},"cell_type":"code","source":"#Rozdzielamy etykiety i cechy\ntrain_data = stratified_train # Kopia\ntest_data = stratified_test\ntrain_data.shape\ntest_data.shape\ntrain_data['deposit'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d3a2d7aae42d41a6d24cb11d2f562c21e3888fb5","_kg_hide-input":false},"cell_type":"code","source":"# Zasadniczo poniższy fragment służy tylko do przekonwertowania wartości opisowych na wartości numeryczne. Teoretycznie dałoby rade to zrobić przy przerobieniu df = df[columns].apply(LabelEncoder().fit_transform), ale wyniki nie były ciekawe.\n# Pobrane z: Hands on Machine Learning with Scikit Learn and Tensorflow; Aurelien Geron.\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy import sparse\n\nclass CategoricalEncoder(BaseEstimator, TransformerMixin):\n    \"\"\"Encode categorical features as a numeric array.\n    The input to this transformer should be a matrix of integers or strings,\n    denoting the values taken on by categorical (discrete) features.\n    The features can be encoded using a one-hot aka one-of-K scheme\n    (``encoding='onehot'``, the default) or converted to ordinal integers\n    (``encoding='ordinal'``).\n    This encoding is needed for feeding categorical data to many scikit-learn\n    estimators, notably linear models and SVMs with the standard kernels.\n    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n    Parameters\n    ----------\n    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n        The type of encoding to use (default is 'onehot'):\n        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n          (or also called 'dummy' encoding). This creates a binary column for\n          each category and returns a sparse matrix.\n        - 'onehot-dense': the same as 'onehot' but returns a dense array\n          instead of a sparse matrix.\n        - 'ordinal': encode the features as ordinal integers. This results in\n          a single column of integers (0 to n_categories - 1) per feature.\n    categories : 'auto' or a list of lists/arrays of values.\n        Categories (unique values) per feature:\n        - 'auto' : Determine categories automatically from the training data.\n        - list : ``categories[i]`` holds the categories expected in the ith\n          column. The passed categories are sorted before encoding the data\n          (used categories can be found in the ``categories_`` attribute).\n    dtype : number type, default np.float64\n        Desired dtype of output.\n    handle_unknown : 'error' (default) or 'ignore'\n        Whether to raise an error or ignore if a unknown categorical feature is\n        present during transform (default is to raise). When this is parameter\n        is set to 'ignore' and an unknown category is encountered during\n        transform, the resulting one-hot encoded columns for this feature\n        will be all zeros.\n        Ignoring unknown categories is not supported for\n        ``encoding='ordinal'``.\n    Attributes\n    ----------\n    categories_ : list of arrays\n        The categories of each feature determined during fitting. When\n        categories were specified manually, this holds the sorted categories\n        (in order corresponding with output of `transform`).\n    Examples\n    --------\n    Given a dataset with three features and two samples, we let the encoder\n    find the maximum value per feature and transform the data to a binary\n    one-hot encoding.\n    >>> from sklearn.preprocessing import CategoricalEncoder\n    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n    ... # doctest: +ELLIPSIS\n    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n              encoding='onehot', handle_unknown='ignore')\n    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n    See also\n    --------\n    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n      integer ordinal features. The ``OneHotEncoder assumes`` that input\n      features take on values in the range ``[0, max(feature)]`` instead of\n      using the unique values.\n    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n      dictionary items (also handles string-valued features).\n    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n      encoding of dictionary items or strings.\n    \"\"\"\n\n    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n                 handle_unknown='error'):\n        self.encoding = encoding\n        self.categories = categories\n        self.dtype = dtype\n        self.handle_unknown = handle_unknown\n\n    def fit(self, X, y=None):\n        \"\"\"Fit the CategoricalEncoder to X.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_feature]\n            The data to determine the categories of each feature.\n        Returns\n        -------\n        self\n        \"\"\"\n\n        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n                        \"or 'ordinal', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.handle_unknown not in ['error', 'ignore']:\n            template = (\"handle_unknown should be either 'error' or \"\n                        \"'ignore', got %s\")\n            raise ValueError(template % self.handle_unknown)\n\n        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n                             \" encoding='ordinal'\")\n\n        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n        n_samples, n_features = X.shape\n\n        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n\n        for i in range(n_features):\n            le = self._label_encoders_[i]\n            Xi = X[:, i]\n            if self.categories == 'auto':\n                le.fit(Xi)\n            else:\n                valid_mask = np.in1d(Xi, self.categories[i])\n                if not np.all(valid_mask):\n                    if self.handle_unknown == 'error':\n                        diff = np.unique(Xi[~valid_mask])\n                        msg = (\"Found unknown categories {0} in column {1}\"\n                               \" during fit\".format(diff, i))\n                        raise ValueError(msg)\n                le.classes_ = np.array(np.sort(self.categories[i]))\n\n        self.categories_ = [le.classes_ for le in self._label_encoders_]\n\n        return self\n\n    def transform(self, X):\n        \"\"\"Transform X using one-hot encoding.\n        Parameters\n        ----------\n        X : array-like, shape [n_samples, n_features]\n            The data to encode.\n        Returns\n        -------\n        X_out : sparse matrix or a 2-d array\n            Transformed input.\n        \"\"\"\n        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n        n_samples, n_features = X.shape\n        X_int = np.zeros_like(X, dtype=np.int)\n        X_mask = np.ones_like(X, dtype=np.bool)\n\n        for i in range(n_features):\n            valid_mask = np.in1d(X[:, i], self.categories_[i])\n\n            if not np.all(valid_mask):\n                if self.handle_unknown == 'error':\n                    diff = np.unique(X[~valid_mask, i])\n                    msg = (\"Found unknown categories {0} in column {1}\"\n                           \" during transform\".format(diff, i))\n                    raise ValueError(msg)\n                else:\n                    # Set the problematic rows to an acceptable value and\n                    # continue `The rows are marked `X_mask` and will be\n                    # removed later.\n                    X_mask[:, i] = valid_mask\n                    X[:, i][~valid_mask] = self.categories_[i][0]\n            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n\n        if self.encoding == 'ordinal':\n            return X_int.astype(self.dtype, copy=False)\n\n        mask = X_mask.ravel()\n        n_values = [cats.shape[0] for cats in self.categories_]\n        n_values = np.array([0] + n_values)\n        indices = np.cumsum(n_values)\n\n        column_indices = (X_int + indices[:-1]).ravel()[mask]\n        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n                                n_features)[mask]\n        data = np.ones(n_samples * n_features)[mask]\n\n        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n                                shape=(n_samples, indices[-1]),\n                                dtype=self.dtype).tocsr()\n        if self.encoding == 'onehot-dense':\n            return out.toarray()\n        else:\n            return out","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da24a63d65c5cf19fcbf5ec57a06e682dae2a0f6","_kg_hide-input":false},"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\n#Wybieranie odpowiednich kolumn na potrzeby konwersji\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n    def fit(self, X, y=None):\n        return self\n    def transform(self, X):\n        return X[self.attribute_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"8d978ce3e64e3d316bcb3e241d4dd9868240adb5"},"cell_type":"code","source":"train_data.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"c6f9d08d4ae5b059d138fbae9c6e48a54d2231c7"},"cell_type":"code","source":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Pipeline'y do przekazywania danych\nnumerical_pipeline = Pipeline([\n    (\"select_numeric\", DataFrameSelector([\"age\", \"balance\", \"day\", \"campaign\", \"pdays\", \"previous\",\"duration\"])),\n    (\"std_scaler\", StandardScaler()),\n])\n\ncategorical_pipeline = Pipeline([\n    (\"select_cat\", DataFrameSelector([\"job\", \"education\", \"marital\", \"default\", \"housing\", \"loan\", \"contact\", \"month\",\n                                     \"poutcome\"])),\n    (\"cat_encoder\", CategoricalEncoder(encoding='onehot-dense'))\n])\n\nfrom sklearn.pipeline import FeatureUnion\npreprocess_pipeline = FeatureUnion(transformer_list=[\n        (\"numerical_pipeline\", numerical_pipeline),\n        (\"categorical_pipeline\", categorical_pipeline),\n    ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7c40f8e6eafdfa5c4b5d2055fa2f368cd4d07308"},"cell_type":"code","source":"X_train = preprocess_pipeline.fit_transform(train_data)\nX_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"80bf8dc8ed2468adb524f8eb2429b9ea27b3cbab"},"cell_type":"code","source":"y_train = train_data['deposit']\ny_test = test_data['deposit']\ny_train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"28210aa6a40a03c9ff177bcc4f932e8f9b541cd6"},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nencode = LabelEncoder()\ny_train = encode.fit_transform(y_train)\ny_test = encode.fit_transform(y_test)\ny_train_yes = (y_train == 1)\ny_train\ny_train_yes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"b7d6a5a7613c9ee1d94aca70a7dc07eca782a63b"},"cell_type":"code","source":"some_instance = X_train[1200]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3d724605d1dd77c122e6a65f2e221ad7aa26ad43"},"cell_type":"code","source":"#Inicjalizacja klasyfikatorów\nimport time\n\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.gaussian_process.kernels import RBF\nimport xgboost\n\n\ndict_classifiers = {\n    \"Logistic Regression\": LogisticRegression(solver='liblinear'),\n    \"XGBoost\": xgboost.XGBClassifier(),\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f1a0b61d476d6bc5d2da3dfb27072d94dddd5585"},"cell_type":"code","source":"# \nno_classifiers = len(dict_classifiers.keys())\n\ndef batch_classify(X_train, Y_train, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['Klasyfikator', 'Skuteczność', 'Czas trenowania'])\n    count = 100\n    for key, classifier in dict_classifiers.items():\n        t_start = time.clock()\n        classifier.fit(X_train, Y_train)\n        t_end = time.clock()\n        t_diff = t_end - t_start\n        train_score = classifier.score(X_train, Y_train)\n        df_results.loc[count,'Klasyfikator'] = key\n        df_results.loc[count,'Skuteczność'] = train_score\n        df_results.loc[count,'Czas trenowania'] = t_diff\n        if verbose:\n            print(\"Wytrenowany {c} w {f:.2f} s\".format(c=key, f=t_diff))\n        count+=1\n    return df_results","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ecf17b0bf54b4624af0732ec34922aa9f9c30608"},"cell_type":"code","source":"df_results = batch_classify(X_train, y_train)\nprint(df_results.sort_values(by='Skuteczność', ascending=False))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b12c215615716b721db29b9ead5e6315be180ec2"},"cell_type":"markdown","source":"### Problem zbytniego dopasowania:\nPoprzez wykorzystanie wielokrotnej walidacji krzyżowej dążymy do tego, aby algorytm podążał według wzoru i nie brał pod uwage \"szumów\"; ma aproksymować wyniki w "},{"metadata":{"trusted":true,"_uuid":"eb72ca11865e63a2bba942afe43a0aa5ccafda3f"},"cell_type":"code","source":"# Walidacja krzyżowa\nfrom sklearn.model_selection import cross_val_score\n\n# Regresja logistyczna\nlog_reg = LogisticRegression(solver='liblinear')\nlog_scores = cross_val_score(log_reg, X_train, y_train, cv=20)\nlog_reg_mean = log_scores.mean()\n\n# XGBoost\nxgb = xgboost.XGBClassifier()\nxgb_scores = cross_val_score(xgb, X_train, y_train, cv=20)\nxgb_mean = xgb_scores.mean()\n\n\n# Dataframe z wynikami\nd = {'Klasyfikatory': ['Regresja Logistyczna','XGB'], \n    'Średnie wyniki': [log_reg_mean, xgb_mean]}\n\nresult_df = pd.DataFrame(data=d)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"438493015531a56fe52feb593861a4cc92c15a6d"},"cell_type":"code","source":"# Wyniki naszych klasyfikatorów\nresult_df = result_df.sort_values(by=['Średnie wyniki'], ascending=False)\nresult_df","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"af5ca2683d0017a628ebd802f281d086f6d356f3","_cell_guid":"b76c68ff-3277-41e0-9397-d9e1ed8a2bca"},"cell_type":"markdown","source":"**Positive/Negative:** Typ decyzji (label) [\"No\", \"Yes\"]\n**True/False:** Dobrze lub źle zakwalifikowane przez model<br><br>\n\n**True Negatives (Lewo - Góra Kwadrat):** \n\n**False Negatives (Prawo - Góra Kwadrat):** \n\n**False Positives (Lewo - Dół Kwadrat):**\n\n**True Positives (Prawo - Dół Kwadrat):** "},{"metadata":{"trusted":true,"_uuid":"9f9645627070ca75abc65db6d99a67aa822e3203"},"cell_type":"code","source":"# Walidacja krzyżowa dla XGBoosta\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(xgb, X_train, y_train, cv=20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0984f57713acf2e96796bc388d680cd29e14c134"},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nxgb.fit(X_train, y_train)\nprint (\"Dokładność XGB wynosi %2.2f\" % accuracy_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Macierz dopasowań\nfrom sklearn.metrics import confusion_matrix\n# 4697: no's, 4232: yes\nconf_matrix = confusion_matrix(y_train, y_train_pred)\nf, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\nplt.title(\"Macierz dopasowań XGB\",fontsize=20)\nplt.subplots_adjust(left=0.15, right=0.99, bottom=0.15, top=0.99)\nax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5, minor=False)\nax.set_xticklabels(\"\")\nax.set_yticklabels(['Odrzucone', 'Zaakceptowane'], fontsize=16, rotation=360)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wartości dla precyzji i zwrotu XGB\nfrom sklearn.metrics import precision_score, recall_score\nprint('Precision Score: ', precision_score(y_train, y_train_pred))\nprint('Recall Score: ', recall_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Walidacja krzyżowa dla LR\nfrom sklearn.model_selection import cross_val_predict\n\ny_train_pred = cross_val_predict(log_reg, X_train, y_train, cv=20)\nfrom sklearn.metrics import accuracy_score\nlog_reg.fit(X_train, y_train)\nprint (\"Dokładność LR wynosi %2.2f\" % accuracy_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a5179f02a1c96fdecf45bf51c4d3ce86ee665da7"},"cell_type":"code","source":"#Macierz dopasowań\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(y_train, y_train_pred)\nf, ax = plt.subplots(figsize=(12, 8))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", linewidths=.5, ax=ax)\nplt.title(\"Macierz dopasowań LR\",fontsize=20)\nplt.subplots_adjust(left=0.15, right=0.99, bottom=0.15, top=0.99)\nax.set_yticks(np.arange(conf_matrix.shape[0]) + 0.5, minor=False)\nax.set_xticklabels(\"\")\nax.set_yticklabels(['Odrzucone', 'Zaakceptowane'], fontsize=16, rotation=360)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Wartości dla precyzji i zwrotu LR\nfrom sklearn.metrics import precision_score, recall_score\nprint('Precision Score: ', precision_score(y_train, y_train_pred))\nprint('Recall Score: ', recall_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import precision_score, recall_score\nf1_score(y_train, y_train_pred)\nprint('Precision Score: ', precision_score(y_train, y_train_pred))\nprint('Recall Score: ', recall_score(y_train, y_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\nf1_score(y_train, y_train_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"036fd09e75d008e7f3873b3bcb8a9128e77e0c09","_cell_guid":"dea3e18f-dd16-40d4-80e1-b47f180bc5d8"},"cell_type":"markdown","source":"# Zwrot i precyzja\n<a id=\"precision_recall\"></a>\n**Zwrot:** Ile decyzji \"yes\" wykrył nasz model <br><br>\n**Precyzja:** Pewność naszego modelu, że dana decyzja to \"yes\n\nZbyt duża precyzja powoduje, że model może na przykładzie poprzednich przykładów przewidzieć, że dana decyzja to \"no\", kiedy tak naprawde to jest \"yes\"."},{"metadata":{"trusted":true},"cell_type":"code","source":"y_scores = log_reg.decision_function([some_instance])\ny_scores","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1838ee7172ea11daa91855495372367c8fa6aa3"},"cell_type":"code","source":"# Zmiana progu dla zwrotów\nthreshold = 0\ny_some_digit_pred = (y_scores > threshold)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Co ma największy wpływ?"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import tree\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nplt.style.use('seaborn-white')\nimport random as rd\n\n#Konwersja\nterm_deposits['job'] = term_deposits['job'].astype('category').cat.codes\nterm_deposits['marital'] = term_deposits['marital'].astype('category').cat.codes\nterm_deposits['education'] = term_deposits['education'].astype('category').cat.codes\nterm_deposits['contact'] = term_deposits['contact'].astype('category').cat.codes\nterm_deposits['poutcome'] = term_deposits['poutcome'].astype('category').cat.codes\nterm_deposits['month'] = term_deposits['month'].astype('category').cat.codes\nterm_deposits['default'] = term_deposits['default'].astype('category').cat.codes\nterm_deposits['loan'] = term_deposits['loan'].astype('category').cat.codes\nterm_deposits['housing'] = term_deposits['housing'].astype('category').cat.codes\n\n#Tworzenie podzbiorów i testy\ntarget_name = 'deposit'\nX = term_deposits.drop('deposit', axis=1)\n\ndef getParam():\n    param = {\n        'eta': rd.random() / 3 + 0.001,            # O ile posuwamy się do przodu po każdej iteracji (nie za duży bo przeskoczymy)\n        'max_depth': rd.randint( 7, 17 ),          # maksymalna głębokość drzewa decyzyjnego\n        'min_child_weight': rd.randint( 1, 10 ),   # Minimalna liczba obserwacji w każdym liściu drzewa\n        'gamma': rd.randint( 0, 6 ),              # Zmniejszenie strat wymaganych do utworzenia kolejnego węzła\n        'n_estimators': rd.randint( 70, 150 ),     # liczba drzewek, które chcemy zbudować\n        'subsample': 1 - rd.random() / 4,          # ile obserwacji bierzemy do budowy drzewka (żeby nie przeuczyć)\n        'colsample_bytree':  1 - rd.random() / 4   # jaki procent charakterystyk chcemy brać do budowy prostego drzewka\n    }\n    return param\nlabel=term_deposits[target_name]\n\nX_train, X_test, y_train, y_test = train_test_split(X,label,test_size=0.2, random_state=42, stratify=label)\n\n#Drzewo decyzyjne\ntree = tree.DecisionTreeClassifier(\n    class_weight='balanced',\n    min_weight_fraction_leaf = 0.01\n)\n    \nresultFile=open('csv_to_submit.csv', \"wt\")\nmaxAccuracy = 0.0\nmaxParam = {}  \nrd.seed()\n\nfor i in range( 100):\n    param = getParam()\n    model = xgboost.XGBClassifier( eta = param[ 'eta' ],\n                max_depth = param[ 'max_depth' ],\n                min_child_weight = param[ 'min_child_weight' ],\n                gamma = param[ 'gamma' ],\n                n_estimators = param[ 'n_estimators' ],\n                subsample = param[ 'subsample' ],\n                colsample_bytree =  param[ 'colsample_bytree' ]\n            )\n    model.fit( X_train, y_train )\n    yPred = model.predict( X_test )\n    ytrain = model.predict( X_train )\n    preds = [  value  for value in yPred ]\n    print( f\"iteration: { i }\\t{ param }\" )\n    accuracy = accuracy_score( y_test, preds )\n    print( \"Accuracy: %.4f%%\" % ( accuracy * 100.0 ) )\n    resultFile.write( f\"For parameters = { param }\\tAccuracy = { accuracy }\\n\" )\n    if maxAccuracy < accuracy:\n        maxAccuracy = accuracy\n        maxParam = param\nresultFile.write( f\"\"\"\\n\\nBest accuracy = { maxAccuracy } for parameters:\\n\n    eta = { maxParam[ 'eta' ] }\n    max_depth = { maxParam[ 'max_depth' ] }\n    min_child_weight = { maxParam[ 'min_child_weight' ] }\n    gamma = { maxParam[ 'gamma' ] }\n    n_estimators = { maxParam[ 'n_estimators' ] }\n    subsample = { maxParam[ 'subsample' ] }\n    colsample_bytree = { maxParam[ 'colsample_bytree' ] }\"\"\" )\nresultFile.close()\n\n#resultFile.to_csv('csv_to_submit.csv', index = False)\n\n#tree = tree.fit(X_train, y_train)\nimportances = model.feature_importances_\nfeature_names = term_deposits.drop('deposit', axis=1).columns\nindices = np.argsort(importances)[::-1]\n\n\nprint(\"Ranking cech:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\ndef feature_importance_graph(indices, importances, feature_names):\n    plt.figure(figsize=(12,6))\n    plt.title(\"Waga parametrów na klasyfikator\", fontsize=18)\n    plt.barh(range(len(indices)), importances[indices], color='#31B173',  align=\"center\")\n    plt.yticks(range(len(indices)), feature_names[indices], rotation='horizontal',fontsize=14)\n    plt.ylim([-1, len(indices)])  \nfeature_importance_graph(indices, importances, feature_names)\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}